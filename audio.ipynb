{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7021860,"sourceType":"datasetVersion","datasetId":4035795}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport torchaudio\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-21T21:33:56.116448Z","iopub.execute_input":"2023-11-21T21:33:56.116850Z","iopub.status.idle":"2023-11-21T21:34:01.444799Z","shell.execute_reply.started":"2023-11-21T21:33:56.116816Z","shell.execute_reply":"2023-11-21T21:34:01.443504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@staticmethod\ndef open(audio_file):\n    try:\n        print(f\"Loading audio file: {audio_file}\")\n        sig, sr = torchaudio.load(audio_file)\n        return (sig, sr)\n    except Exception as e:\n        print(f\"Error loading audio file {audio_file}: {str(e)}\")\n        return None\nopen(\"/kaggle/input/catmeows-classification/catmeows/catmeows/B_ANI01_MC_FN_SIM01_101.wav\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:34:05.992622Z","iopub.execute_input":"2023-11-21T21:34:05.993251Z","iopub.status.idle":"2023-11-21T21:34:06.114111Z","shell.execute_reply.started":"2023-11-21T21:34:05.993211Z","shell.execute_reply":"2023-11-21T21:34:06.113114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import pandas as pd\n\n# # Define the folder names and their corresponding labels\n# label_mapping = {\n#     'HuntingMind': 1,\n#     'Resting': 2,\n#     'Warning': 3,\n#     'Angry': 4,\n#     'Paining': 5,\n#     'Happy': 6\n# }\n\n# # Set the root directory\n# root_dir = \"/kaggle/input/catmeows-classification/dataset/dataset/raw dataset\"\n\n# # Initialize lists to store file paths and corresponding labels\n# file_paths = []\n# labels = []\n\n# # Traverse through the directories\n# for label, index in label_mapping.items():\n#     label_dir = os.path.join(root_dir, label)\n#     for file_name in os.listdir(label_dir):\n#         if file_name.endswith('.mp3'):\n#             file_path = os.path.abspath(os.path.join(label_dir, file_name))  # Get absolute path\n#             file_paths.append(file_path)\n#             labels.append(index)\n\n# # Create a DataFrame\n# data = {'File_Path': file_paths, 'Label': labels}\n# df = pd.DataFrame(data)\n\n# # Save the DataFrame to a CSV file with absolute paths\n# csv_filename = \"/kaggle/working/audio_labels_absolute.csv\"\n# df.to_csv(csv_filename, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.576891Z","iopub.execute_input":"2023-11-21T21:07:32.577273Z","iopub.status.idle":"2023-11-21T21:07:32.582457Z","shell.execute_reply.started":"2023-11-21T21:07:32.577239Z","shell.execute_reply":"2023-11-21T21:07:32.581515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import pandas as pd\n\n# # Path to the directory containing the audio files\n# audio_dir = \"/kaggle/input/catmeows-classification/catmeows/catmeows/\"\n\n# # Initialize lists to store data\n# file_paths = []\n# labels = []\n\n# # Mapping for label values\n# label_mapping = {\n#     'B': 1,  # Brushing\n#     'F': 2,  # Waiting for food\n#     'I': 3   # Isolation in an unfamiliar environment\n# }\n\n# # Iterate through audio files\n# for filename in os.listdir(audio_dir):\n#     if filename.endswith(\".wav\"):\n#         # Extract information from the filename\n#         parts = filename.split(\"_\")\n#         emission_context = parts[0][0]\n#         cat_id = parts[1]\n#         label = label_mapping[emission_context]\n\n#         # Construct the absolute path\n#         file_path = os.path.join(audio_dir, filename)\n\n#         # Append data to lists\n#         file_paths.append(file_path)\n#         labels.append(label)\n\n# # Create a DataFrame\n# df = pd.DataFrame({'absolute_path': file_paths, 'Label': labels})\n\n# # Export to CSV\n# csv_path = \"/kaggle/working/catmeows_dataset.csv\"\n# df.to_csv(csv_path, index=False)\n\n# print(f\"CSV file exported to: {csv_path}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.584582Z","iopub.execute_input":"2023-11-21T21:07:32.584845Z","iopub.status.idle":"2023-11-21T21:07:32.594859Z","shell.execute_reply.started":"2023-11-21T21:07:32.584822Z","shell.execute_reply":"2023-11-21T21:07:32.593955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read metadata file\nmetadata_file = \"/kaggle/input/catmeows-classification/catmeows/catmeows/catmeows_dataset.csv\"\ndf = pd.read_csv(metadata_file)\n\n# Construct absolute file path\ndf['absolute_path'] = df['absolute_path']\n\n# Take relevant columns\ndf = df[['absolute_path', 'Label']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.595914Z","iopub.execute_input":"2023-11-21T21:07:32.596167Z","iopub.status.idle":"2023-11-21T21:07:32.640104Z","shell.execute_reply.started":"2023-11-21T21:07:32.596143Z","shell.execute_reply":"2023-11-21T21:07:32.639220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math, random\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\nfrom IPython.display import Audio\n\nclass AudioUtil():\n  # ----------------------------\n  # Load an audio file. Return the signal as a tensor and the sample rate\n  # ----------------------------\n  @staticmethod\n  def open(audio_file):\n    sig, sr = torchaudio.load(audio_file)\n    return (sig, sr)\n\n  # ----------------------------\n  # Convert the given audio to the desired number of channels\n  # ----------------------------\n  @staticmethod\n  def rechannel(aud, new_channel):\n    sig, sr = aud\n\n    if (sig.shape[0] == new_channel):\n      # Nothing to do\n      return aud\n\n    if (new_channel == 1):\n      # Convert from stereo to mono by selecting only the first channel\n      resig = sig[:1, :]\n    else:\n      # Convert from mono to stereo by duplicating the first channel\n      resig = torch.cat([sig, sig])\n\n    return ((resig, sr))\n\n  # ----------------------------\n  # Since Resample applies to a single channel, we resample one channel at a time\n  # ----------------------------\n  @staticmethod\n  def resample(aud, newsr):\n    sig, sr = aud\n\n    if (sr == newsr):\n      # Nothing to do\n      return aud\n\n    num_channels = sig.shape[0]\n    # Resample first channel\n    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n    if (num_channels > 1):\n      # Resample the second channel and merge both channels\n      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n      resig = torch.cat([resig, retwo])\n\n    return ((resig, newsr))\n\n  # ----------------------------\n  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n  # ----------------------------\n  @staticmethod\n  def pad_trunc(aud, max_ms):\n    sig, sr = aud\n    num_rows, sig_len = sig.shape\n    max_len = sr//1000 * max_ms\n\n    if (sig_len > max_len):\n      # Truncate the signal to the given length\n      sig = sig[:,:max_len]\n\n    elif (sig_len < max_len):\n      # Length of padding to add at the beginning and end of the signal\n      pad_begin_len = random.randint(0, max_len - sig_len)\n      pad_end_len = max_len - sig_len - pad_begin_len\n\n      # Pad with 0s\n      pad_begin = torch.zeros((num_rows, pad_begin_len))\n      pad_end = torch.zeros((num_rows, pad_end_len))\n\n      sig = torch.cat((pad_begin, sig, pad_end), 1)\n      \n    return (sig, sr)\n\n  # ----------------------------\n  # Shifts the signal to the left or right by some percent. Values at the end\n  # are 'wrapped around' to the start of the transformed signal.\n  # ----------------------------\n  @staticmethod\n  def time_shift(aud, shift_limit):\n    sig,sr = aud\n    _, sig_len = sig.shape\n    shift_amt = int(random.random() * shift_limit * sig_len)\n    return (sig.roll(shift_amt), sr)\n\n  # ----------------------------\n  # Generate a Spectrogram\n  # ----------------------------\n  @staticmethod\n  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n    sig,sr = aud\n    top_db = 80\n\n    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n\n    # Convert to decibels\n    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n    return (spec)\n\n  # ----------------------------\n  # Augment the Spectrogram by masking out some sections of it in both the frequency\n  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n  # overfitting and to help the model generalise better. The masked sections are\n  # replaced with the mean value.\n  # ----------------------------\n  @staticmethod\n  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n    _, n_mels, n_steps = spec.shape\n    mask_value = spec.mean()\n    aug_spec = spec\n\n    freq_mask_param = max_mask_pct * n_mels\n    for _ in range(n_freq_masks):\n      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n    time_mask_param = max_mask_pct * n_steps\n    for _ in range(n_time_masks):\n      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n\n    return aug_spec","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.641237Z","iopub.execute_input":"2023-11-21T21:07:32.641568Z","iopub.status.idle":"2023-11-21T21:07:32.660484Z","shell.execute_reply.started":"2023-11-21T21:07:32.641543Z","shell.execute_reply":"2023-11-21T21:07:32.659601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\n# ----------------------------\n# Sound Dataset\n# ----------------------------\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n\n    # ----------------------------\n    # Number of items in dataset\n    # ----------------------------\n    def __len__(self):\n        return len(self.df)\n\n    # ----------------------------\n    # Get i'th item in dataset\n    # ----------------------------\n    def __getitem__(self, idx):\n        # Absolute file path of the audio file - use the 'absolute_path' column\n        audio_file = self.df.loc[idx, 'absolute_path']\n        # Get the Class ID\n        class_id = self.df.loc[idx, 'Label']\n\n        aud = AudioUtil.open(audio_file)\n        # Some sounds have a higher sample rate, or fewer channels compared to the\n        # majority. So make all sounds have the same number of channels and same \n        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n        # result in arrays of different lengths, even though the sound duration is\n        # the same.\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n\n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n        return aug_sgram, class_id\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.661656Z","iopub.execute_input":"2023-11-21T21:07:32.662032Z","iopub.status.idle":"2023-11-21T21:07:32.674902Z","shell.execute_reply.started":"2023-11-21T21:07:32.661997Z","shell.execute_reply":"2023-11-21T21:07:32.674067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Sound Dataset\n# ----------------------------\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n\n    # ----------------------------\n    # Number of items in dataset\n    # ----------------------------\n    def __len__(self):\n        return len(self.df)\n\n    # ----------------------------\n    # Get i'th item in dataset\n    # ----------------------------\n    def __getitem__(self, idx):\n        # Absolute file path of the audio file - use the 'absolute_path' column\n        audio_file = self.df.loc[idx, 'absolute_path']\n        # Get the Class ID\n        class_id = self.df.loc[idx, 'Label']\n\n        aud = AudioUtil.open(audio_file)\n        # Some sounds have a higher sample rate, or fewer channels compared to the\n        # majority. So make all sounds have the same number of channels and same \n        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n        # result in arrays of different lengths, even though the sound duration is\n        # the same.\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n\n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n        return aug_sgram, class_id\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.676126Z","iopub.execute_input":"2023-11-21T21:07:32.676523Z","iopub.status.idle":"2023-11-21T21:07:32.691632Z","shell.execute_reply.started":"2023-11-21T21:07:32.676488Z","shell.execute_reply":"2023-11-21T21:07:32.690907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ndata_path = \"/kaggle/input/catmeows-classification/catmeows/catmeows\"\nmyds = SoundDS(df, data_path)\n\n# Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.692705Z","iopub.execute_input":"2023-11-21T21:07:32.693019Z","iopub.status.idle":"2023-11-21T21:07:32.709169Z","shell.execute_reply.started":"2023-11-21T21:07:32.692986Z","shell.execute_reply":"2023-11-21T21:07:32.708472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n# ----------------------------\n# Audio Classification Model\n# ----------------------------\nclass AudioClassifier(nn.Module):\n    # ----------------------------\n    # Build the model architecture\n    # ----------------------------\n    def __init__(self):\n        super().__init__()\n        conv_layers = []\n\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        # Second Convolution Block\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n\n        # Third Convolution Block\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        # Fourth Convolution Block\n        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Linear Classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n#         self.dropout = nn.Dropout(p=dropout_rate)\n        self.lin = nn.Linear(in_features=64, out_features=10)\n\n        # Wrap the Convolutional Blocks\n        self.conv = nn.Sequential(*conv_layers)\n \n    # ----------------------------\n    # Forward pass computations\n    # ----------------------------\n    def forward(self, x):\n        # Run the convolutional blocks\n        x = self.conv(x)\n\n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n\n        # Linear layer\n        x = self.lin(x)\n\n        # Final output\n        return x\n\n# Create the model and put it on the GPU if available\n# dropout_rate = 0.5\nmyModel = AudioClassifier()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = myModel.to(device)\n# Check that it is on Cuda\nnext(myModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:32.711756Z","iopub.execute_input":"2023-11-21T21:07:32.712047Z","iopub.status.idle":"2023-11-21T21:07:35.975150Z","shell.execute_reply.started":"2023-11-21T21:07:32.712017Z","shell.execute_reply":"2023-11-21T21:07:35.974191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Training Loop\n# ----------------------------\ndef training(model, train_dl, num_epochs):\n  # Loss Function, Optimizer and Scheduler\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(),lr=0.004)\n#   optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n#   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.004,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n    \n\n  # Repeat for each epoch\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Repeat for each batch in the training set\n    for i, data in enumerate(train_dl):\n        # Get the input features and target labels, and put them on the GPU\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Normalize the inputs\n        inputs_m, inputs_s = inputs.mean(), inputs.std()\n        inputs = (inputs - inputs_m) / inputs_s\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Keep stats for Loss and Accuracy\n        running_loss += loss.item()\n\n        # Get the predicted class with the highest score\n        _, prediction = torch.max(outputs,1)\n        # Count of predictions that matched the target label\n        correct_prediction += (prediction == labels).sum().item()\n        total_prediction += prediction.shape[0]\n\n        #if i % 10 == 0:    # print every 10 mini-batches\n        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n    \n    # Print stats at the end of the epoch\n    num_batches = len(train_dl)\n    avg_loss = running_loss / num_batches\n    acc = correct_prediction/total_prediction\n    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n  print('Finished Training')\n  \nnum_epochs=30   # Just for demo, adjust this higher.\ntraining(myModel, train_dl, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:07:35.976375Z","iopub.execute_input":"2023-11-21T21:07:35.976701Z","iopub.status.idle":"2023-11-21T21:09:04.114479Z","shell.execute_reply.started":"2023-11-21T21:07:35.976674Z","shell.execute_reply":"2023-11-21T21:09:04.113576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# After the training loop\ndef evaluate(model, dataloader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return all_preds, all_labels\n\n# Evaluate the model on the validation set\nval_preds, val_labels = evaluate(myModel, val_dl, device)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(val_labels, val_preds)\n\n# Print or plot the confusion matrix\nclass_names = [str(i) for i in range(1, 4)]  # Classes 1, 2, 3\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:09:39.611208Z","iopub.execute_input":"2023-11-21T21:09:39.611927Z","iopub.status.idle":"2023-11-21T21:09:41.396455Z","shell.execute_reply.started":"2023-11-21T21:09:39.611892Z","shell.execute_reply":"2023-11-21T21:09:41.395474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------\n# Inference\n# ----------------------------\ndef inference (model, val_dl):\n  correct_prediction = 0\n  total_prediction = 0\n\n  # Disable gradient updates\n  with torch.no_grad():\n    for data in val_dl:\n      # Get the input features and target labels, and put them on the GPU\n      inputs, labels = data[0].to(device), data[1].to(device)\n\n      # Normalize the inputs\n      inputs_m, inputs_s = inputs.mean(), inputs.std()\n      inputs = (inputs - inputs_m) / inputs_s\n\n      # Get predictions\n      outputs = model(inputs)\n\n      # Get the predicted class with the highest score\n      _, prediction = torch.max(outputs,1)\n      # Count of predictions that matched the target label\n      correct_prediction += (prediction == labels).sum().item()\n      total_prediction += prediction.shape[0]\n    \n  acc = correct_prediction/total_prediction\n  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n\n# Run inference on trained model with the validation set\ninference(myModel, val_dl)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T21:09:04.115538Z","iopub.execute_input":"2023-11-21T21:09:04.115804Z","iopub.status.idle":"2023-11-21T21:09:05.212363Z","shell.execute_reply.started":"2023-11-21T21:09:04.115781Z","shell.execute_reply":"2023-11-21T21:09:05.211328Z"},"trusted":true},"execution_count":null,"outputs":[]}]}